#pragma once
#include "n00b.h"
/*
 * This is not necessarily the final algorithm, just initial notes
 * that aren't yet fully consistent, since this is a work in progress.
 *
 * My goal here is to support a very simple garbage collection scheme
 * where individual threads can control their own garbage collection,
 * but still hold cross-thread references.
 *
 * The basic approach is that each thread gets its own set of memory
 * arenas that it allocates from. It only allocates from a single
 * arena at a time, and can add new arenas when it runs out of room.
 *
 * It can also decide when to 'compact' its arenas, at which point it
 * marks each arena as 'collecting'. It creates a new arena to copy
 * live data into. We take a very straightforward copy-collector
 * approach with our own heap, treating the arena's list of
 * cross-references as roots; forwarding information is written INTO
 * the cross-reference, in case the other thread decides to drop the
 * reference before its next access.
 *
 * Currently, cross-thread references cannot occur at the same time as
 * a migration. Essentially, there is a single per-allocation
 * write-lock that doesn't affect in-thread writes to the thread's
 * arenas.
 *
 * As of now, we use spin locks for this. A thread holds the lock if
 * it writes its own ID into the lock successfully via CAS, and it
 * spins until it is successful. Once it's successful, it performs its
 * action, and then unlocks.
 *
 * This means that only one thread can be writing to any given
 * thread's heap at a time.
 *
 * If a thread decides it wants to compact, but cannot obtain the
 * lock, it writes a bit into the lock to indicate that no
 * cross-thread references may grab the lock, even if they win, so
 * that it takes priority over other thread writes.
 *
 * When a thread is compacting and we need to update pointers across
 * threads, we do not reach into the heaps of the other threads. We
 * instead update the other thread's registration in our heap. That
 * way, the other thread can dangle the thread and blow away the
 * arena, without us risking a bad write. These objects are reference
 * counted, and the reference count will never be above 2.
 *
 * Note that there's also the notion of a 'global' arena, where
 * allocations are *never* compacted. This is meant only to hold
 * references to our lock-free data structures that are shared across
 * threads, without having to go through the pain of locking
 * arenas. The intent is that this is for instantiation of
 * module-level (or global) values.
 *
 * For instance, let's say there's a module-level dictionary, used by
 * all threads, but thread 1 atomically changes the reference stored
 * in the module. Other threads may have copied the reference to the
 * obejct, but the object will have been allocated in the global
 * arena, so all of those copies will incref the underlying data
 * object, so that it won't be freed until all references have been
 * decref'd, and the count reaches 0.
 *
 * This is error prone if done manually, and is meant more for code
 * generated by the compiler.
 */

// In the future, we would expect that a writer seeing the
// 'collecting' field will attempt to help migration to minimize
// time spent waiting, but for the time being, any cross-thread
// writes to a thread-local heap will involve spinning.
#define GC_FLAG_COLLECTING 0x00000001

// Whether collection has reached this allocation yet.
#define GC_FLAG_REACHED 0x00000002

// Whether collection has finished migrating this allocation and all it's
// dependencies.
#define GC_FLAG_MOVED 0x00000004

// Whether another thread is currently mutating a cross-thread pointer.
#define GC_FLAG_WRITER_LOCK 0x00000008

// True when the owner is waiting to use the value.
#define GC_FLAG_OWNER_WAITING 0x00000010

// True when we are freezing everything to marshal memory in toto.
#define GC_FLAG_GLOBAL_STOP 0x00000020

// Shouldn't be accessed by developer, but allows us to inline.
extern uint64_t     n00b_gc_guard;
extern n00b_arena_t *n00b_new_arena(size_t, hatrack_zarray_t *);
extern void         n00b_delete_arena(n00b_arena_t *);
extern void         n00b_expand_arena(size_t, n00b_arena_t **);
extern n00b_arena_t *n00b_collect_arena(n00b_arena_t *);
extern void        *n00b_gc_resize(void *ptr, size_t len);
extern void         n00b_gc_thread_collect();
extern bool         n00b_is_read_only_memory(volatile void *);
extern void         n00b_gc_set_finalize_callback(n00b_system_finalizer_fn);

typedef struct n00b_segment_range_t {
    void                       *start;
    void                       *end;
    struct n00b_segment_range_t *next;
    int                         segment_id;
} n00b_segment_range_t;

extern _Atomic(n00b_segment_range_t *) n00b_static_segments;

#ifdef N00B_ADD_ALLOC_LOC_INFO
extern void _n00b_arena_register_root(n00b_arena_t *,
                                     void *,
                                     uint64_t,
                                     char *,
                                     int);
extern void _n00b_gc_register_root(void *, uint64_t, char *f, int l);

#define n00b_arena_register_root(x, y, z) \
    _n00b_arena_register_root(x, y, z, __FILE__, __LINE__)
#define n00b_gc_register_root(x, y) \
    _n00b_gc_register_root(x, y, __FILE__, __LINE__)

#else
extern void _n00b_arena_register_root(n00b_arena_t *, void *, uint64_t);
extern void _n00b_gc_register_root(void *, uint64_t);

#define n00b_arena_register_root(x, y, z) _n00b_arena_register_root(x, y, z)
#define n00b_gc_register_root(x, y)       _n00b_gc_register_root(x, y)
#endif

void n00b_arena_remove_root(n00b_arena_t *arena, void *ptr);
void n00b_gc_remove_root(void *ptr);

#ifdef N00B_ADD_ALLOC_LOC_INFO
extern void *_n00b_gc_raw_alloc(size_t, n00b_mem_scan_fn, char *, int);
extern void *_n00b_gc_raw_alloc_with_finalizer(size_t,
                                              n00b_mem_scan_fn,
                                              char *,
                                              int);

#define n00b_gc_raw_alloc(x, y) \
    _n00b_gc_raw_alloc(x, y, __FILE__, __LINE__)
#define n00b_gc_raw_alloc_with_finalizer(x, y) \
    _n00b_gc_raw_alloc_with_finalizer(x, y, __FILE__, __LINE__)

extern void *n00b_alloc_from_arena(n00b_arena_t **,
                                  size_t,
                                  n00b_mem_scan_fn,
                                  bool,
                                  char *,
                                  int);

extern n00b_utf8_t *n00b_gc_alloc_info(void *, int *);
#else
extern void *_n00b_gc_raw_alloc(size_t, n00b_mem_scan_fn);
extern void *_n00b_gc_raw_alloc_with_finalizer(size_t, n00b_mem_scan_fn);

#define n00b_gc_raw_alloc_with_finalizer(x, y) \
    _n00b_gc_raw_alloc_with_finalizer(x, y)
#define n00b_gc_raw_alloc(x, y) _n00b_gc_raw_alloc(x, y)
extern void *n00b_alloc_from_arena(n00b_arena_t **,
                                  size_t,
                                  n00b_mem_scan_fn,
                                  bool);
#endif

extern int n00b_gc_show_heap_stats_on;

#ifdef N00B_GC_STATS
#define n00b_gc_show_heap_stats_on()  n00b_gc_show_heap_stats_on = 1
#define n00b_gc_show_heap_stats_off() n00b_gc_show_heap_stats_on = 0
#else
#define n00b_gc_show_heap_stats_on()
#define n00b_gc_show_heap_stats_off()
#endif

#ifdef N00B_GC_FULL_TRACE
extern int n00b_gc_trace_on;

#define n00b_gc_gen_trace_implementation(X)
#define n00b_gc_trace(X, ...)                            \
    {                                                   \
        if (X && n00b_gc_trace_on) {                     \
            fprintf(stderr, "gc_trace:%s: ", __func__); \
            fprintf(stderr, __VA_ARGS__);               \
            fputc('\n', stderr);                        \
        }                                               \
    }

#define n00b_trace_on()  n00b_gc_trace_on = 1
#define n00b_trace_off() n00b_gc_trace_on = 0
#else
#define n00b_trace_on()
#define n00b_trace_off()
#define n00b_gc_trace(...)
#endif

static inline uint64_t
n00b_round_up_to_given_power_of_2(uint64_t power, uint64_t n)
{
    uint64_t modulus   = (power - 1);
    uint64_t remainder = n & modulus;

    if (!remainder) {
        return n;
    }
    else {
        return (n & ~modulus) + power;
    }
}

#define N00B_GC_SCAN_ALL  ((void *)0)
#define N00B_GC_SCAN_NONE ((void *)0xffffffffffffffff)

// gc_malloc and gc_alloc_* should only be used for INTERNAL dynamic
// allocations. Anything that would be exposed to the language user
// should be allocated via `gc_new()`, because there's an expectation
// of an `object` header.
#define n00b_gc_malloc(l) n00b_gc_raw_alloc(l, N00B_GC_SCAN_ALL)

#define n00b_gc_flex_alloc(fixed, var, numv, map) \
    (n00b_gc_raw_alloc((size_t)(sizeof(fixed)) + (sizeof(var)) * (numv), (map)))

#define n00b_gc_alloc_mapped(typename, map) \
    (n00b_gc_raw_alloc(sizeof(typename), (void *)map))

#define n00b_gc_alloc(typename) \
    (n00b_gc_raw_alloc(sizeof(typename), N00B_GC_SCAN_ALL))

#define n00b_gc_value_alloc(typename) \
    (n00b_gc_raw_alloc(sizeof(typename), N00B_GC_SCAN_NONE))

// Assumes it contains pointers. Call manually if you need otherwise.
#define n00b_gc_array_alloc(typename, n) \
    n00b_gc_raw_alloc((sizeof(typename) * n), N00B_GC_SCAN_ALL)

#define n00b_gc_array_value_alloc(typename, n) \
    n00b_gc_raw_alloc((sizeof(typename) * n), N00B_GC_SCAN_NONE)

#define n00b_gc_array_alloc_mapped(typename, n, map) \
    n00b_gc_raw_alloc((sizeof(typename) * n), (void *)map)

typedef void (*n00b_gc_hook)();

extern void           n00b_initialize_gc();
extern void           n00b_gc_heap_stats(uint64_t *, uint64_t *, uint64_t *);
extern void           n00b_gc_add_hold(n00b_obj_t);
extern void           n00b_gc_remove_hold(n00b_obj_t);
extern n00b_arena_t   *n00b_internal_stash_heap();
extern void           n00b_internal_unstash_heap();
extern void           n00b_internal_set_heap(n00b_arena_t *);
extern void           n00b_internal_lock_then_unstash_heap();
extern void           n00b_get_heap_bounds(uint64_t *, uint64_t *, uint64_t *);
extern void           n00b_gc_register_collect_fns(n00b_gc_hook, n00b_gc_hook);
extern n00b_alloc_hdr *n00b_find_alloc(void *);
extern bool           n00b_in_heap(void *);
extern void           n00b_add_static_segment(void *, void *);
// TODO: Keep a list of all heaps.

#ifdef N00B_GC_STATS
uint64_t n00b_get_alloc_counter();
#else
#define n00b_get_alloc_counter() (0)
#endif

#ifdef N00B_FULL_MEMCHECK
void n00b_alloc_display_front_guard_error(n00b_alloc_hdr *, void *, char *, int, bool);
void n00b_alloc_display_rear_guard_error(n00b_alloc_hdr *, void *, int, void *, char *, int, bool);

void _n00b_memcheck_raw_alloc(void *, char *, int);
void _n00b_memcheck_object(n00b_obj_t, char *, int);
#define n00b_memcheck_raw_alloc(x) \
    _n00b_memcheck_raw_alloc(((void *)x), __FILE__, __LINE__);
#define n00b_memcheck_object(x) \
    _n00b_memcheck_object(((void *)x), __FILE__, __LINE__);
#else
#define n00b_memcheck_raw_alloc(x)
#define n00b_memcheck_object(x)
#endif

#if defined(__n00b_have_asan__)
void __asan_poison_memory_region(void const volatile *addr, size_t size);
void __asan_unpoison_memory_region(void const volatile *addr, size_t size);

#define ASAN_POISON_MEMORY_REGION(addr, size) \
    __asan_poison_memory_region((addr), (size))
#define ASAN_UNPOISON_MEMORY_REGION(addr, size) \
    __asan_unpoison_memory_region((addr), (size))
#else
#define ASAN_POISON_MEMORY_REGION(addr, size) \
    ((void)(addr), (void)(size))
#define ASAN_UNPOISON_MEMORY_REGION(addr, size) \
    ((void)(addr), (void)(size))
#endif

static inline ptrdiff_t
n00b_ptr_diff(void *base, void *field)
{
    return (int64_t *)field - (int64_t *)base;
}

static inline void
n00b_mark_raw_to_addr(uint64_t *bitfield, void *base, void *end)
{
    uint64_t diff = n00b_ptr_diff(base, end);

    while (diff >= 64) {
        *bitfield++ = ~0ULL;
        diff -= 64;
    }
    *bitfield = (1ULL << (diff + 1)) - 1;
}

static inline n00b_alloc_hdr *
n00b_object_header(n00b_obj_t o)
{
    n00b_mem_ptr p = {.v = o};
    p.alloc -= 1;

    return p.alloc;
}

extern void *n00b_raw_arena_alloc(uint64_t, void **, void **);
